{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_SPAeSFX5wWQ",
    "ExecuteTime": {
     "end_time": "2023-07-11T13:39:19.380110699Z",
     "start_time": "2023-07-11T13:39:19.335500032Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import yaml\n",
    "# import dcargs\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Setting default colormap settings\n",
    "from palettable.scientific.diverging import Berlin_3\n",
    "from palettable.lightbartlein.diverging import BlueOrange10_2, BlueOrange12_2, BlueOrangeRed_2\n",
    "from palettable.matplotlib import Viridis_3\n",
    "CMap = Viridis_3.get_mpl_colormap()\n",
    "\n",
    "def get_colorMaps(nIds):\n",
    "    weights = np.arange(0, nIds)\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=nIds)\n",
    "    cmap = mpl.cm.ScalarMappable(norm=norm, cmap=CMap)\n",
    "\n",
    "    return cmap\n",
    "\n",
    "# path = \"/Users/mohsen/Desktop/project/colab/with vito/new Results\"\n",
    "# path = \"../results/hpc/central_network\"\n",
    "# path = \"../results/hpc/spatial_network\"\n",
    "# path = \"../results/hpc/spatial_network/individual_runs/envstd_0.0001_mnmax_0.0001_mnmin_0.0001_np1_0.2631578947368421_ow_0.0_a_n_100_run_0\"\n",
    "path = \"../results/N100_2023-05-02-14-33-16_test_grid_search_Naive_centralized_random/results/envstd_7_mnint_0_mnmin_0_np1_0_ow_0_a_n_0_run_4\"\n",
    "sys.path.append(path)\n",
    "\n",
    "# # path = \"/home/mohsen/Project/colab/collective-decison-making-with-direl/results/2022-08-05-17-08-11_basic_experiment\"\n",
    "\n",
    "# path = \"/home/mohsen/Project/colab/collective-decison-making-with-direl/results/2022-10-26-13-39-34_test_grid_search_naive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:39:21.639331432Z",
     "start_time": "2023-07-11T13:39:21.632545063Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_experiment_data_par' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_48568/2695008871.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# SHITTY INTROSPEKTIONEN\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdata_post_pro\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparse_experiment_data_par\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'parse_experiment_data_par' is not defined"
     ]
    }
   ],
   "source": [
    "# SHITTY INTROSPEKTIONEN\n",
    "\n",
    "data_post_pro = parse_experiment_data_par(path)\n",
    "\n",
    "\n",
    "# data_path = path + \"/\" +  \"data.pickle5\"\n",
    "\n",
    "# with open(data_path, \"rb\") as f:\n",
    "#     df_bayes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:46.995551179Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import yaml\n",
    "import dcargs\n",
    "import pandas as pd\n",
    "\n",
    "from experiment import ExperimentParameters\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from yaml.loader import SafeLoader\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# def parse_experiment_data_par(file_path):\n",
    "file_path = path\n",
    "if (True):\n",
    "    # print(\"File Path is: \" + file_path)\n",
    "    # try:\n",
    "    if(True):\n",
    "        pickleFileName = file_path+\"/data.pickle5\"\n",
    "        yamlFileName = file_path+\"/config.yaml\"\n",
    "\n",
    "        with open(pickleFileName, \"rb\") as f:\n",
    "            resFile = pickle.load(f)\n",
    "\n",
    "        with open(yamlFileName, 'r') as f:\n",
    "            configData = dcargs.from_yaml(ExperimentParameters, f)\n",
    "\n",
    "\n",
    "        n_agents = configData.network_params.n_agents\n",
    "        steps = configData.steps + 1\n",
    "        z_gt = configData.true_value\n",
    "\n",
    "        # not checked yet\n",
    "        range_agent_measurement_noise = configData.network_params.max_agent_measurement_noise - configData.network_params.min_agent_measurement_noise\n",
    "        mean_agent_measurement_noise = 0.5*(configData.network_params.max_agent_measurement_noise + configData.network_params.min_agent_measurement_noise)\n",
    "        env_noise_std = configData.env_noise_std\n",
    "        scalar_param1 = configData.network_params.scalar_param1\n",
    "        weight_own_belief = configData.network_params.weight_own_belief\n",
    "\n",
    "        colData = resFile['collective_data']\n",
    "        adjc = colData['connectivity']\n",
    "        agntData = colData['agent_data']\n",
    "\n",
    "        # print(\"nAgents: \" + str(n_agents) + \", steps: \" + str(steps));\n",
    "        # print(\"agnt data: \", len(agntData))\n",
    "\n",
    "        zArr = np.zeros((n_agents,steps))\n",
    "        pArr = zArr.copy()\n",
    "\n",
    "        for i in range(n_agents):\n",
    "            z = agntData[i].belief_mean\n",
    "            std = agntData[i].belief_std\n",
    "            zArr[i,:] = z\n",
    "            pArr[i,:] = std\n",
    "\n",
    "        avgZArr = np.mean(zArr, axis=0)\n",
    "        E_t = (avgZArr-z_gt)**2\n",
    "        E_p2 = (zArr - avgZArr)**2\n",
    "        E_p = np.mean(E_p2, axis=0)\n",
    "        E_a = E_t + E_p\n",
    "\n",
    "        G = nx.from_numpy_array(adjc[0].cpu().detach().numpy())\n",
    "        degreeDist = np.array(G.degree())[:, 1]\n",
    "        clustCoef = np.array(list(nx.clustering(G).values()))\n",
    "        eigVec = np.array(list(nx.eigenvector_centrality_numpy(G).values()))\n",
    "\n",
    "\n",
    "        GG = nx.adjacency_matrix(G).toarray()\n",
    "        row_sums = GG.sum(axis=1)\n",
    "        row_sums[row_sums==0] = 1\n",
    "        row_sums[np.isinf(row_sums)] = 1\n",
    "        G2 = GG / row_sums[:, np.newaxis]\n",
    "        eigVal = np.linalg.eigvals(G2)\n",
    "        eigVal[np.isinf(eigVal)] = np.iinfo(np.int16).max\n",
    "        # eigVal[np.isinf(eigVal) + np.isnan(eigVal)] = -1.01\n",
    "        # eigVal = np.sort(np.abs(eigVal))\n",
    "\n",
    "        # print(\"Start Testing Adjc Over Time! for steps: \", steps)\n",
    "        eigvals_time = []\n",
    "        # adjc_time = []\n",
    "        # for t in range(steps):\n",
    "        #     # print(t)\n",
    "        #     G = nx.from_numpy_array(adjc[t].cpu().detach().numpy())\n",
    "        #     GG = nx.adjacency_matrix(G).toarray()\n",
    "        #     nx.draw(G)\n",
    "        #     # plt.imshow(GG)\n",
    "        #     # plt.show()\n",
    "        #     row_sums = GG.sum(axis=1)\n",
    "        #     row_sums[row_sums==0] = 1\n",
    "        #     row_sums[np.isinf(row_sums)] = 1\n",
    "        #     G2 = GG / row_sums[:, np.newaxis]\n",
    "        #     # print(\"row sums: \",row_sums, flush=True)\n",
    "        #     # print(\"/n\")\n",
    "        #     eigVal = np.linalg.eigvals(G2)\n",
    "        #     # eigVal[np.isinf(eigVal) + np.isnan(eigVal)] = -1.01\n",
    "        #     eigVal[np.isinf(eigVal)] = np.iinfo(np.int16).max\n",
    "        #     # L = nx.normalized_laplacian_matrix(G)\n",
    "        #     # eigVal = np.array(list(nx.eigenvector_centrality(G).values()))\n",
    "        #     # print(\"eigVals lenght: \", eigVal.shape, flush=True)\n",
    "        #     eigvals_time.append(eigVal)\n",
    "        #     # adjc_time.append(adjc[t].cpu().detach().numpy())\n",
    "\n",
    "        # print(\"eigVals list lenght: \", len(eigvals_time), flush=True)\n",
    "\n",
    "        d = {\"Trueness_Error\": E_t, \"Precision_Error\": E_p, \"Accuracy_Error\": E_a, \"Collective_Mean\": avgZArr,\n",
    "            \"Number_of_Agents\": [n_agents for _ in range(steps)], \"True_Value\": [z_gt for _ in range(steps)],\n",
    "            \"Timestep\": [i for i in range(steps)],\n",
    "            \"mean_agent_measurement_noise\": [mean_agent_measurement_noise for _ in range(steps)],\n",
    "            \"range_agent_measurement_noise\": [range_agent_measurement_noise for _ in range(steps)],\n",
    "            \"std_environment_noise\": [env_noise_std for _ in range(steps)],\n",
    "            \"scalar_param1\": [scalar_param1 for _ in range(steps)],\n",
    "            \"weight_own_belief\": [weight_own_belief for _ in range(steps)],\n",
    "             # ToDo: Calculate Network properties for each time-step, so that we can see the evolution of these properties over time\n",
    "            \"Netw_num_of_Edges\" : G.number_of_edges(),\n",
    "            \"Netw_std_degree\" : np.std(degreeDist),\n",
    "            \"Netw_mean_degree\" : np.mean(degreeDist),\n",
    "            \"Netw_std_eigVec\" : np.std(eigVec),\n",
    "            \"Netw_mean_eigVec\" : np.mean(eigVec),\n",
    "            \"Netw_max_eigVec\" : np.max(eigVec),\n",
    "            \"Netw_std_CC\" : np.std(clustCoef),\n",
    "            \"Netw_mean_CC\" : np.mean(clustCoef),\n",
    "            \"Netw_max_CC\" : np.max(clustCoef),\n",
    "            \"Netw_std_eigVal\" : np.std(eigVal),\n",
    "            \"Netw_mean_eigVal\" : np.mean(eigVal),\n",
    "            \"Netw_max_eigVal\" : np.max(eigVal),#[-1],\n",
    "            # \"Netw_2ndmax_eigVal\" : eigVal[-2],\n",
    "            # \"Netw_Adjc\" : adjc_time,\n",
    "            \"Netw_eigVals_vs_time\" : eigvals_time\n",
    "            }\n",
    "            # \"Netw_eigVals_vs_time\" : [get_eigVals_of_adj_mat(nx.from_numpy_array(adjc[i].cpu().detach().numpy())) for i in range(steps)]}\n",
    "\n",
    "        # return pd.DataFrame.from_dict(d)\n",
    "    # except Exception as e:\n",
    "    else:\n",
    "        print(e,flush=True)\n",
    "        print(\"Something Went Wrong!\",flush=True)\n",
    "        print(\"File Path is: \" + file_path,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.002587739Z"
    }
   },
   "outputs": [],
   "source": [
    "pArr[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.002879928Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(zArr)\n",
    "avgZArr = np.mean(zArr, axis=1)\n",
    "upBnd = avgZArr + 3*np.std(zArr, axis=1)\n",
    "loBnd = avgZArr - 3*np.std(zArr, axis=1)\n",
    "plt.plot(avgZArr,\"--b\")\n",
    "plt.plot(upBnd,\"--r\", loBnd, \"--r\",lw=0.5)\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"mean ind. belief\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.003115872Z"
    }
   },
   "outputs": [],
   "source": [
    "G."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Bayes DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.003333881Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# experiment_name = \"N25_2022-12-28-06-44-28_test_grid_search_Bayes\"\n",
    "# experiment_name = \"N50_2022-12-28-13-53-37_test_grid_search_Bayes\"\n",
    "# experiment_name = \"N100_2022-12-24-13-06-21_test_grid_search_Bayes\"\n",
    "# experiment_name = \"N200_2022-12-27-17-25-46_test_grid_search_Bayes\"\n",
    "\n",
    "\n",
    "# with central network\n",
    "experiment_name = \"N100_2023-02-01-17-52-23_test_grid_search_Bayes_centralized_random\"\n",
    "\n",
    "# Spatial network\n",
    "experiment_name = \"N100_2023-02-03-12-16-56_test_grid_search_Bayes_spatial\"\n",
    "\n",
    "experiment_folder_path = path + \"/\" + experiment_name \n",
    "\n",
    "# dataFrame_path = path + \"/\" + experiment_name + \"__df_withNet_overTime.pickle\"\n",
    "# dataFrame_path = path + \"/\" + experiment_name + \"__df_withNet_overTime_WithAdjc.pickle\"\n",
    "# dataFrame_path = path + \"/\" + experiment_name + \"__df_withNet_overTime_updatedEigs.pickle\"\n",
    "dataFrame_path = path + \"/\" + experiment_name + \"__df_withNet_overTime_updatedEigs.pickle\"\n",
    "\n",
    "\n",
    "with open(dataFrame_path, \"rb\") as f:\n",
    "    df_bayes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the eigen values data from the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.003526553Z"
    }
   },
   "outputs": [],
   "source": [
    "df_bayes.groupby([\"Timestep\"])\n",
    "# ,\"Netw_eigVals_vs_time\"]\n",
    "mean_eig = []\n",
    "std_eig = []\n",
    "scnd_eig = []\n",
    "time_list = []\n",
    "scnd_eig_ratio = []\n",
    "scnd_eig_t0 = 1\n",
    "for i in range(len(df_bayes)): # range(10000):#\n",
    "    eigVals = df_bayes.loc[i,'Netw_eigVals_vs_time']\n",
    "    eigVals = np.sort(np.abs(eigVals))\n",
    "    # print(eigVals[-2])\n",
    "    # plt.plot(df_bayes.loc[i,'Timestep'], eigVals[-2],'.')\n",
    "    # plt.plot(df_bayes.loc[i,'Timestep'], np.mean(eigVals),'.')\n",
    "    time_list.append(df_bayes.loc[i,'Timestep'])\n",
    "    mean_eig.append(np.mean(np.abs(eigVals)))\n",
    "    std_eig.append(np.std(np.abs(eigVals)))\n",
    "    scnd_eig_val = eigVals[-2]\n",
    "    scnd_eig.append(scnd_eig_val)\n",
    "\n",
    "    if(df_bayes.loc[i,'Timestep']==0):\n",
    "        scnd_eig_t0 = scnd_eig_val\n",
    "    scnd_eig_ratio.append(scnd_eig_val/scnd_eig_t0)\n",
    "\n",
    "    if(df_bayes.loc[i,'Timestep']==10):\n",
    "        time_list.append(np.nan)\n",
    "        mean_eig.append(np.nan)\n",
    "        std_eig.append(np.nan)\n",
    "        scnd_eig.append(np.nan)\n",
    "        scnd_eig_ratio.append(np.nan)\n",
    "\n",
    "# plt.plot(np.array(time_list),np.array(mean_eig))\n",
    "# plt.plot(np.array(time_list),np.array(scnd_eig),lw=0.1)\n",
    "    # print(df_bayes.loc[i,'Timestep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigen values over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.004070545Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(time_list),np.array(mean_eig),lw=0.1)\n",
    "plt.ylabel(\"mean eigs\")\n",
    "plt.show()\n",
    "plt.plot(np.array(time_list),np.array(std_eig),lw=0.01)\n",
    "plt.ylabel(\"std eigs\")\n",
    "plt.show()\n",
    "plt.plot(np.array(time_list),np.abs(np.array(scnd_eig)),lw=0.1)\n",
    "plt.ylabel(\"2nd eigs\")\n",
    "plt.show()\n",
    "fig = plt.figure()\n",
    "plt.plot(np.array(time_list),np.abs(np.array(scnd_eig_ratio)),lw=0.1)\n",
    "plt.plot(np.array([0, 9]),np.array([1, 1]),lw=0.5,color='r')\n",
    "plt.ylabel(\"2nd eigs/2nd eig @ t0\")\n",
    "fig.set_facecolor(\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check any parameter of the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.004255092Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_bayes.groupby([\"Timestep\"])\n",
    "# # ,\"Netw_eigVals_vs_time\"]\n",
    "# mean_eig = []\n",
    "# std_eig = []\n",
    "# scnd_eig = []\n",
    "# time_list = []\n",
    "# scnd_eig_ratio = []\n",
    "# scnd_eig_t0 = 1\n",
    "# for i in range(len(df_bayes)): # range(10000):#\n",
    "#     eigVals = df_bayes.loc[i,'Netw_eigVals_vs_time']\n",
    "#     eigVals = np.sort(np.abs(eigVals))\n",
    "#     # print(eigVals[-2])\n",
    "#     # plt.plot(df_bayes.loc[i,'Timestep'], eigVals[-2],'.')\n",
    "#     # plt.plot(df_bayes.loc[i,'Timestep'], np.mean(eigVals),'.')\n",
    "#     time_list.append(df_bayes.loc[i,'Timestep'])\n",
    "#     mean_eig.append(np.mean(np.abs(eigVals)))\n",
    "#     std_eig.append(np.std(np.abs(eigVals)))\n",
    "#     scnd_eig_val = eigVals[-2]\n",
    "#     scnd_eig.append(scnd_eig_val)\n",
    "\n",
    "#     if(df_bayes.loc[i,'Timestep']==0):\n",
    "#         scnd_eig_t0 = scnd_eig_val\n",
    "#     scnd_eig_ratio.append(scnd_eig_val/scnd_eig_t0)\n",
    "\n",
    "#     if(df_bayes.loc[i,'Timestep']==9):\n",
    "#         time_list.append(np.nan)\n",
    "#         mean_eig.append(np.nan)\n",
    "#         std_eig.append(np.nan)\n",
    "#         scnd_eig.append(np.nan)\n",
    "#         scnd_eig_ratio.append(np.nan)\n",
    "\n",
    "# # plt.plot(np.array(time_list),np.array(mean_eig))\n",
    "# # plt.plot(np.array(time_list),np.array(scnd_eig),lw=0.1)\n",
    "#     # print(df_bayes.loc[i,'Timestep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.004446722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking a few math operations on network weights\n",
    "\n",
    "import networkx as nx\n",
    "G = nx.from_numpy_array(df_bayes.loc[50,\"Netw_Adjc\"])\n",
    "# nx.draw(G)\n",
    "GG = nx.adjacency_matrix(G).toarray()\n",
    "row_sums = GG.sum(axis=1)\n",
    "row_sums[row_sums==0] = 1\n",
    "G2 = GG / row_sums[:, np.newaxis]\n",
    "# print(GG.sum(axis=1))\n",
    "# G2 = GG/GG.sum(axis=0)\n",
    "# print(G2.sum(axis=1))\n",
    "# print(np.sum(G2[1,:]))\n",
    "# print(G2)\n",
    "# print(G2[1,:])\n",
    "# L = nx.normalized_laplacian_matrix(G)\n",
    "# eigVal = np.linalg.eigvals(L.A)\n",
    "\n",
    "eigVals = np.linalg.eigvals(G2)\n",
    "print(eigVals)\n",
    "plt.plot(np.sort(eigVals))\n",
    "\n",
    "\n",
    "a = np.arange(0,27,3).reshape(3,3)\n",
    "row_sums = a.sum(axis=1)\n",
    "new_matrix = a / row_sums[:, np.newaxis]\n",
    "print(a)\n",
    "print(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.004742467Z"
    }
   },
   "outputs": [],
   "source": [
    "centrality = np.array(list(nx.eigenvector_centrality(G).values()))\n",
    "print(centrality)\n",
    "plt.plot(np.sort(centrality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-11T13:38:47.005145905Z"
    }
   },
   "outputs": [],
   "source": [
    "eigs = df_bayes.loc[4500,'Netw_eigVals_vs_time']\n",
    "plt.plot(np.sort(eigs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = np.random.uniform(0,1,10)\n",
    "row_sums[4]=np.Inf\n",
    "row_sums[row_sums==0] = 1\n",
    "print(row_sums)\n",
    "print(np.isinf(row_sums) + np.isnan(row_sums))\n",
    "\n",
    "# eigVal[np.isinf(eigVal) + np.isnan(eigVal)] = -1\n",
    "print(row_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = df_bayes.loc[4500,'Netw_eigVals_vs_time']\n",
    "plt.plot(np.sort(eigs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Naive dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name_naive = \"N25_2022-12-27-19-04-42_test_grid_search_Naive\"\n",
    "# experiment_name_naive = \"N50_2022-12-23-17-32-03_test_grid_search_Naive\"\n",
    "# experiment_name_naive = \"N100_2022-12-23-17-36-29_test_grid_search_Naive\"\n",
    "experiment_name_naive = \"N200_2022-12-23-17-35-36_test_grid_search_Naive\"\n",
    "\n",
    "# Central networks\n",
    "experiment_name_naive = \"N100_2023-02-02-14-46-01_test_grid_search_Naive_centralized_random\"\n",
    "\n",
    "# Spatial Networks\n",
    "experiment_name_naive = \"N100_2023-02-03-12-15-22_test_grid_search_Naive_spatial\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "experiment_folder_path_naive = path + \"/\" + experiment_name_naive \n",
    "\n",
    "dataFrame_path_naive = path + \"/\" + experiment_name_naive + \"__df_withNet_updatedEigs.pickle\"\n",
    "\n",
    "with open(dataFrame_path_naive, \"rb\") as f:\n",
    "    df_naive = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the Bayes DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmu = 0.1\n",
    "df_bayes_filtered = df_bayes; #df[df['mean agent measurement noise']==mmu]\n",
    "# df_bayes_filtered = df[df['mean agent measurement noise']==mmu]\n",
    "# time_index = 9\n",
    "# df_bayes_filtered = df_bayes_filtered[df_bayes_filtered['Timestep']==time_index]\n",
    "# n_agents = 100#32\n",
    "# df_bayes_filtered = df_bayes_filtered[df_bayes_filtered['Number_of_Agents']==n_agents]\n",
    "range_smallest = df_bayes_filtered['range_agent_measurement_noise'].min()\n",
    "df_bayes_filtered = df_bayes_filtered[df_bayes_filtered['range_agent_measurement_noise']==range_smallest]\n",
    "df_bayes_filtered = df_bayes_filtered[df_bayes_filtered['std_environment_noise']==df_bayes_filtered['mean_agent_measurement_noise']+range_smallest*0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the Naive DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmu = 0.1\n",
    "df_naive_filtered = df_naive; #df[df['mean agent measurement noise']==mmu]\n",
    "# df_filtered = df[df['mean agent measurement noise']==mmu]\n",
    "time_index = 9\n",
    "df_naive_filtered = df_naive_filtered[df_naive_filtered['Timestep']==time_index]\n",
    "# df_naive_filtered = df_naive_filtered[df_naive_filtered['Number_of_Agents']==n_agents]\n",
    "# df_naive_filtered = df_naive_filtered[df_naive_filtered['std environment noise']==df_naive_filtered['mean agent measurement noise']+range_smallest*0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_naive_filtered[\"Collective_Mean\"].max())\n",
    "fig = plt.figure(1,figsize=(14,10))\n",
    "fig.set_facecolor('w')\n",
    "\n",
    "ax1 = plt.subplot(2,3,1); ax1.set_title(\"Trueness Error\")\n",
    "df_bayes_filtered.plot.hexbin(x=\"std environment noise\", y=\"scalar param1\", C=\"Trueness Error\", reduce_C_function=np.mean, gridsize=15, ax=ax1);\n",
    "\n",
    "ax5 = plt.subplot(2,3,2); ax5.set_title(\"Precision Error\")\n",
    "df_bayes_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\", C=\"Precision Error\", reduce_C_function=np.mean, gridsize=15, ax=ax5);\n",
    "\n",
    "ax8 = plt.subplot(2,3,3); ax8.set_title(\"Accuracy Error\")\n",
    "df_bayes_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\", C=\"Accuracy Error\", reduce_C_function=np.mean, gridsize=15, ax=ax8);\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(2,3,4); ax1.set_title(\"Trueness Error\")\n",
    "df_naive_filtered.plot.hexbin(x=\"std environment noise\", y=\"scalar param1\", C=\"Trueness Error\", reduce_C_function=np.mean, gridsize=15, ax=ax1);\n",
    "\n",
    "ax5 = plt.subplot(2,3,5); ax5.set_title(\"Precision Error\")\n",
    "df_naive_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\", C=\"Precision Error\", reduce_C_function=np.mean, gridsize=15, ax=ax5);\n",
    "\n",
    "ax8 = plt.subplot(2,3,6); ax8.set_title(\"Accuracy Error\")\n",
    "df_naive_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\", C=\"Accuracy Error\", reduce_C_function=np.mean, gridsize=15, ax=ax8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a specifc instance of dataframe by defining scalar param 1, std env noise, mean agent noise, num agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_to_plot_bayes = df_bayes['Trueness Error'].mean()\n",
    "scalar_param_vals = np.unique(df_bayes[\"scalar_param1\"])\n",
    "# weight_own_belief_vals = np.unique(df_naive.weight_own_belief)\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "fig.set_size_inches(9,9)\n",
    "# str_to_plot = \"Accuracy\" # \"Precision\" # \"Trueness\" # \n",
    "str_to_plot = \"Precision\" # \"Trueness\" #  \n",
    "# str_to_plot = \"Trueness\" #  \n",
    "# scalar_param_to_check = scalar_param_vals[19]#1.0\n",
    "for scalar_param_to_check in scalar_param_vals:#[10:11]:\n",
    "    std_environment_noise_to_check = np.unique(df_bayes[\"std_environment_noise\"])[-1] # 1.0#0.0001\n",
    "    # num_agents_to_check = 200\n",
    "    df_bayes_filtered = df_bayes\n",
    "    df_bayes_filtered = df_bayes_filtered[df_bayes_filtered[\"scalar_param1\"]==scalar_param_to_check]\n",
    "    df_bayes_filtered = df_bayes_filtered[df_bayes_filtered[\"std_environment_noise\"]==std_environment_noise_to_check]# and\n",
    "    # df_bayes_filtered = df_bayes_filtered[df_bayes_filtered[\"mean agent measurement noise\"]==std_environment_noise_to_check]\n",
    "    # df_bayes_filtered = df_bayes_filtered[df_bayes_filtered[\"Number_of_Agents\"]==num_agents_to_check]\n",
    "    \n",
    "    df_naive_filtered = df_naive\n",
    "    df_naive_filtered = df_naive_filtered[df_naive_filtered[\"scalar_param1\"]==scalar_param_to_check]\n",
    "    df_naive_filtered = df_naive_filtered[df_naive_filtered[\"std_environment_noise\"]==std_environment_noise_to_check]# and\n",
    "    df_naive_filtered = df_naive_filtered[df_naive_filtered[\"mean_agent_measurement_noise\"]==std_environment_noise_to_check]\n",
    "    # df_naive_filtered = df_naive_filtered[df_naive_filtered[\"Number_of_Agents\"]==num_agents_to_check]\n",
    "    # df_naive_filtered = df_naive_filtered[df_naive_filtered[\"weight_own_belief\"]==weight_own_belief_vals[0]]\n",
    "    \n",
    "    if(str_to_plot==\"Trueness\"):\n",
    "        axs[0].plot(df_bayes_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"],color=CMap(scalar_param_to_check),label=\"E_T (B)\")\n",
    "        axs[1].plot(df_naive_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"],color=CMap(scalar_param_to_check),label=\"E_T (N)\")\n",
    "        difference = df_bayes_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"]-df_naive_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"]\n",
    "        axs[2].plot(difference.to_numpy(),color=CMap(scalar_param_to_check),label=\"E_T (B-N)\")\n",
    "\n",
    "    elif(str_to_plot==\"Precision\"):\n",
    "        axs[0].plot(df_bayes_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"],color=CMap(scalar_param_to_check),label=\"E_P (B)\")\n",
    "        axs[1].plot(df_naive_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"],color=CMap(scalar_param_to_check),label=\"E_P (N)\") #,color=\"b\"\n",
    "        difference = df_bayes_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"]-df_naive_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"]\n",
    "        axs[2].plot(difference.to_numpy(),color=CMap(scalar_param_to_check),label=\"E_P (B-N)\")\n",
    "\n",
    "    elif(str_to_plot==\"Accuracy\"):\n",
    "        axs[0].plot(df_bayes_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"],color=CMap(scalar_param_to_check),label=\"E_A (B)\")\n",
    "        axs[1].plot(df_naive_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"],color=CMap(scalar_param_to_check),label=\"E_A (N)\") #,color=\"b\"\n",
    "        difference = df_bayes_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"]-df_naive_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"]\n",
    "        axs[2].plot(difference.to_numpy(),color=CMap(scalar_param_to_check),label=\"E_A (B-N)\")\n",
    "\n",
    "\n",
    "\n",
    "if(str_to_plot==\"Trueness\"):\n",
    "    axs[0].set_ylabel(\"$E_T$ for Bayes\")\n",
    "    axs[1].set_ylabel(\"$E_T$ for Naiive\")\n",
    "    axs[2].set_ylabel(\"$E_T$ (B-N)\")\n",
    "\n",
    "elif(str_to_plot==\"Precision\"):\n",
    "    axs[0].set_ylabel(\"$E_P$ for Bayes\")\n",
    "    axs[1].set_ylabel(\"$E_P$ for Naiive\")\n",
    "    axs[2].set_ylabel(\"$E_P$ (B-N)\")\n",
    "\n",
    "elif(str_to_plot==\"Accuracy\"):\n",
    "    axs[0].set_ylabel(\"$E_A$ for Bayes\")\n",
    "    axs[1].set_ylabel(\"$E_A$ for Naiive\")\n",
    "    axs[2].set_ylabel(\"$E_A$ (B-N)\")\n",
    "\n",
    "axs[1].sharey(axs[0])\n",
    "axs[2].set_xlabel(\"Time Step\")\n",
    "\n",
    "axs[0].set_title(str_to_plot + \" Error\")\n",
    "\n",
    "axs[0].set_yscale('log') \n",
    "axs[1].set_yscale('log') \n",
    "\n",
    "fig.set_facecolor(\"w\")\n",
    "fig.savefig(path + str_to_plot+\"_logy.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bayes[df_bayes[\"Timestep\"]==9].groupby(\"scalar_param1\")[\"Trueness_Error\"].mean().plot(color='b')\n",
    "df_naive[df_naive[\"Timestep\"]==9].groupby(\"scalar_param1\")[\"Trueness_Error\"].mean().plot(color='r')\n",
    "\n",
    "# ax = plt.gca()\n",
    "# ax.set_face_color(\"w\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_bayes[df_bayes[\"Timestep\"] == 9].groupby(\"scalar_param1\")[\"Precision_Error\"].mean().plot(color='b')\n",
    "df_naive[df_naive[\"Timestep\"] == 9].groupby(\"scalar_param1\")[\"Precision_Error\"].mean().plot(color='r')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_bayes[df_bayes[\"Timestep\"] == 9].groupby(\"scalar_param1\")[\"Netw_num_of_Edges\"].mean().plot(color='b')\n",
    "df_naive[df_naive[\"Timestep\"] == 9].groupby(\"scalar_param1\")[\"Netw_num_of_Edges\"].mean().plot(color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_to_plot_bayes = df_bayes['Trueness Error'].mean()\n",
    "scalar_param_vals = np.unique(df_naive[\"scalar_param1\"])\n",
    "# weight_own_belief_vals = np.unique(df_naive.weight_own_belief)\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "fig.set_size_inches(9,9)\n",
    "str_to_plot = \"Accuracy\" # \"Precision\" # \"Trueness\" # \n",
    "# str_to_plot = \"Precision\" # \"Trueness\" #  \n",
    "# str_to_plot = \"Trueness\" #  \n",
    "# scalar_param_to_check = scalar_param_vals[19]#1.0\n",
    "for scalar_param_to_check in scalar_param_vals:#[10:11]:\n",
    "    df_naive_filtered = df_naive\n",
    "    df_naive_filtered = df_naive_filtered[df_naive_filtered[\"scalar_param1\"]==scalar_param_to_check]\n",
    "    # df_naive_filtered = df_naive_filtered[df_naive_filtered[\"std_environment_noise\"]==std_environment_noise_to_check]# and\n",
    "    # df_naive_filtered = df_naive_filtered[df_naive_filtered[\"mean_agent_measurement_noise\"]==std_environment_noise_to_check]\n",
    "    # df_naive_filtered = df_naive_filtered[df_naive_filtered[\"Number_of_Agents\"]==num_agents_to_check]\n",
    "    # df_naive_filtered = df_naive_filtered[df_naive_filtered[\"weight_own_belief\"]==weight_own_belief_vals[0]]\n",
    "    \n",
    "    if(str_to_plot==\"Trueness\"):\n",
    "        # axs[0].plot(df_bayes_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"],color=CMap(scalar_param_to_check),label=\"E_T (B)\")\n",
    "        axs[1].plot(df_naive_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"],color=CMap(scalar_param_to_check),label=\"E_T (N)\")\n",
    "        # difference = df_bayes_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"]-df_naive_filtered.groupby(\"Timestep\").mean()[\"Trueness_Error\"]\n",
    "        # axs[2].plot(difference.to_numpy(),color=CMap(scalar_param_to_check),label=\"E_T (B-N)\")\n",
    "\n",
    "    elif(str_to_plot==\"Precision\"):\n",
    "        # axs[0].plot(df_bayes_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"],color=CMap(scalar_param_to_check),label=\"E_P (B)\")\n",
    "        axs[1].plot(df_naive_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"],color=CMap(scalar_param_to_check),label=\"E_P (N)\") #,color=\"b\"\n",
    "        # difference = df_bayes_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"]-df_naive_filtered.groupby(\"Timestep\").mean()[\"Precision_Error\"]\n",
    "        # axs[2].plot(difference.to_numpy(),color=CMap(scalar_param_to_check),label=\"E_P (B-N)\")\n",
    "\n",
    "    elif(str_to_plot==\"Accuracy\"):\n",
    "        # axs[0].plot(df_bayes_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"],color=CMap(scalar_param_to_check),label=\"E_A (B)\")\n",
    "        axs[1].plot(df_naive_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"],color=CMap(scalar_param_to_check),label=\"E_A (N)\") #,color=\"b\"\n",
    "        # difference = df_bayes_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"]-df_naive_filtered.groupby(\"Timestep\").mean()[\"Accuracy_Error\"]\n",
    "        # axs[2].plot(difference.to_numpy(),color=CMap(scalar_param_to_check),label=\"E_A (B-N)\")\n",
    "\n",
    "\n",
    "\n",
    "if(str_to_plot==\"Trueness\"):\n",
    "    axs[0].set_ylabel(\"$E_T$ for Bayes\")\n",
    "    axs[1].set_ylabel(\"$E_T$ for Naiive\")\n",
    "    axs[2].set_ylabel(\"$E_T$ (B-N)\")\n",
    "\n",
    "elif(str_to_plot==\"Precision\"):\n",
    "    axs[0].set_ylabel(\"$E_P$ for Bayes\")\n",
    "    axs[1].set_ylabel(\"$E_P$ for Naiive\")\n",
    "    axs[2].set_ylabel(\"$E_P$ (B-N)\")\n",
    "\n",
    "elif(str_to_plot==\"Accuracy\"):\n",
    "    axs[0].set_ylabel(\"$E_A$ for Bayes\")\n",
    "    axs[1].set_ylabel(\"$E_A$ for Naiive\")\n",
    "    axs[2].set_ylabel(\"$E_A$ (B-N)\")\n",
    "\n",
    "axs[1].sharey(axs[0])\n",
    "axs[2].set_xlabel(\"Time Step\")\n",
    "\n",
    "axs[0].set_title(str_to_plot + \" Error\")\n",
    "\n",
    "axs[0].set_yscale('log') \n",
    "axs[1].set_yscale('log') \n",
    "\n",
    "fig.set_facecolor(\"w\")\n",
    "fig.savefig(path + str_to_plot+\"_logy.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_bayes[\"std environment noise\"])[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = CMap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,1.5))\n",
    "col_map = plt.get_cmap(cmap)\n",
    "mpl.colorbar.ColorbarBase(ax, cmap=col_map, orientation = 'horizontal')\n",
    "ax.set_xlabel(\"Scalar param1\",fontdict={'size':20})\n",
    "\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "\n",
    "fig.set_facecolor(\"w\")\n",
    "fig.savefig(path+\"ColorMap.png\",dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(difference.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_naive_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bayes.groupby(\"scalar param1\")[\"scalar param1\"]\n",
    "# print(np.unique(df_bayes[\"scalar param1\"]))\n",
    "# print(np.unique(df_bayes[\"std environment noise\"]))\n",
    "print(np.unique(df_naive_filtered.weight_own_belief))\n",
    "# print(np.unique(df_bayes[\"Number of Agents\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bayes[\"mean agent measurement noise\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_bayes[['Trueness Error', \"Timestep\"]].groupby(\"Timestep\").mean()\n",
    "# x = df_bayes[['Trueness Error', \"Timestep\"]].groupby(\"Timestep\").std()\n",
    "print(x)\n",
    "\n",
    "y = df_naive[['Trueness Error', \"Timestep\"]].groupby(\"Timestep\").mean()\n",
    "# y = df_naive[['Trueness Error', \"Timestep\"]].groupby(\"Timestep\").std()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.sort(df_bayes_filtered[\"std environment noise\"]))\n",
    "# plt.plot(np.sort(df_naive_filtered[\"std environment noise\"]))\n",
    "\n",
    "plt.hist(df_naive_filtered[\"scalar_param1\"])\n",
    "# plt.hist(df_bayes_filtered[\"std environment noise\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Manual version of visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(14,10))\n",
    "fig.set_facecolor('w')\n",
    "\n",
    "# ax1 = plt.subplot(2,3,1); ax1.set_title(\"Trueness Error\")\n",
    "df_bayes_filtered.plot.hexbin(x=\"std environment noise\", y=\"scalar param1\",C=\"Trueness Error\", reduce_C_function=np.mean, gridsize=15);#, ax=ax1);\n",
    "\n",
    "# ax5 = plt.subplot(2,3,2); ax5.set_title(\"Precision Error\")\n",
    "# df_bayes_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\",C=\"Precision Error\", reduce_C_function=np.mean, gridsize=15, ax=ax5);\n",
    "\n",
    "# ax8 = plt.subplot(2,3,3); ax8.set_title(\"Accuracy Error\")\n",
    "# df_bayes_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\",C=\"Accuracy Error\", reduce_C_function=np.mean, gridsize=15, ax=ax8);\n",
    "\n",
    "\n",
    "# ax1 = plt.subplot(2,3,4); ax1.set_title(\"Trueness Error\")\n",
    "# df_naive_filtered.plot.hexbin(x=\"std environment noise\", y=\"scalar param1\",C=\"Trueness Error\", reduce_C_function=np.mean, gridsize=15, ax=ax1);\n",
    "\n",
    "# ax5 = plt.subplot(2,3,5); ax5.set_title(\"Precision Error\")\n",
    "# df_naive_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\",C=\"Precision Error\", reduce_C_function=np.mean, gridsize=15, ax=ax5);\n",
    "\n",
    "# ax8 = plt.subplot(2,3,6); ax8.set_title(\"Accuracy Error\")\n",
    "# df_naive_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\",C=\"Accuracy Error\", reduce_C_function=np.mean, gridsize=15, ax=ax8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(15,14))\n",
    "fig.set_facecolor('w')\n",
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "ax1 = plt.subplot(3,3,1); ax1.set_title(\"Trueness Error\")\n",
    "df_filtered.plot.hexbin(x=\"mean agent measurement noise\",   y=\"scalar param1\",C=\"Trueness Error\", reduce_C_function=np.mean, gridsize=15, ax=ax1);\n",
    "ax2 = plt.subplot(3,3,2); ax2.set_title(\"Trueness Error\")\n",
    "df_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\",C=\"Trueness Error\", reduce_C_function=np.mean, gridsize=15, ax=ax2);\n",
    "ax3 = plt.subplot(3,3,3); ax3.set_title(\"Trueness Error\")\n",
    "df_filtered.plot.hexbin(x=\"mean agent measurement noise\",   y=\"std environment noise\", C=\"Trueness Error\", reduce_C_function=np.mean, gridsize=15, ax=ax3);\n",
    "# df_filtered.plot.hexbin(x=\"scalar param1\",                  y=\"scalar param1\",C=\"Trueness Error\",  gridsize=15, ax=ax3);\n",
    "\n",
    "ax4 = plt.subplot(3,3,4); ax4.set_title(\"Precision Error\")\n",
    "df_filtered.plot.hexbin(x=\"mean agent measurement noise\",   y=\"scalar param1\",C=\"Precision Error\", reduce_C_function=np.mean, gridsize=15, ax=ax4);\n",
    "ax5 = plt.subplot(3,3,5); ax5.set_title(\"Precision Error\")\n",
    "df_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\",C=\"Precision Error\", reduce_C_function=np.mean, gridsize=15, ax=ax5);\n",
    "ax6 = plt.subplot(3,3,6); ax6.set_title(\"Precision Error\")\n",
    "df_filtered.plot.hexbin(x=\"mean agent measurement noise\",   y=\"std environment noise\", C=\"Precision Error\", reduce_C_function=np.mean, gridsize=15, ax=ax6);\n",
    "# df_filtered.plot.hexbin(x=\"scalar param1\",                  y=\"scalar param1\",C=\"Precision Error\", reduce_C_function=np.max, gridsize=15, ax=ax6);\n",
    "\n",
    "ax7 = plt.subplot(3,3,7); ax7.set_title(\"Accuracy Error\")\n",
    "df_filtered.plot.hexbin(x=\"mean agent measurement noise\",   y=\"scalar param1\",C=\"Accuracy Error\", reduce_C_function=np.mean, gridsize=15, ax=ax7);\n",
    "ax8 = plt.subplot(3,3,8); ax8.set_title(\"Accuracy Error\")\n",
    "df_filtered.plot.hexbin(x=\"std environment noise\",    y=\"scalar param1\",C=\"Accuracy Error\", reduce_C_function=np.mean, gridsize=15, ax=ax8);\n",
    "ax9 = plt.subplot(3,3,9); ax8.set_title(\"Accuracy Error\")\n",
    "df_filtered.plot.hexbin(x=\"mean agent measurement noise\",   y=\"std environment noise\", C=\"Accuracy Error\", reduce_C_function=np.mean, gridsize=15, ax=ax9);\n",
    "# df_filtered.plot.hexbin(x=\"scalar param1\",                  y=\"scalar param1\",C=\"Accuracy Error\", reduce_C_function=np.max, gridsize=15, ax=ax9);\n",
    "\n",
    "# df_filtered.plot.hexbin(x=\"std environment noise\", y=\"scalar param1\", C=\"Precision Error\", reduce_C_function=np.max, gridsize=15, ax=ax3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(1,figsize=(15,12))\n",
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "ax1 = plt.subplot(3,3,1)\n",
    "df_filtered.plot.scatter(x=\"mean agent measurement noise\",   y=\"scalar param1\",c=\"Trueness Error\",ax=ax1);\n",
    "ax2 = plt.subplot(3,3,2)\n",
    "df_filtered.plot.scatter(x=\"std environment noise\",    y=\"scalar param1\",c=\"Trueness Error\",  ax=ax2);\n",
    "ax3 = plt.subplot(3,3,3)\n",
    "df_filtered.plot.scatter(x=\"mean agent measurement noise\",   y=\"std environment noise\", c=\"Trueness Error\",   ax=ax3);\n",
    "# df_filtered.plot.hexbin(x=\"scalar param1\",                  y=\"scalar param1\",C=\"Trueness Error\",  ax=ax3);\n",
    "\n",
    "ax4 = plt.subplot(3,3,4)\n",
    "df_filtered.plot.scatter(x=\"mean agent measurement noise\",   y=\"scalar param1\",c=\"Precision Error\",  ax=ax4);\n",
    "ax5 = plt.subplot(3,3,5)\n",
    "df_filtered.plot.scatter(x=\"std environment noise\",    y=\"scalar param1\",c=\"Precision Error\",  ax=ax5);\n",
    "ax6 = plt.subplot(3,3,6)\n",
    "df_filtered.plot.scatter(x=\"mean agent measurement noise\",   y=\"std environment noise\", c=\"Precision Error\",  ax=ax6);\n",
    "# df_filtered.plot.hexbin(x=\"scalar param1\",                  y=\"scalar param1\",C=\"Precision Error\",  ax=ax6);\n",
    "\n",
    "ax7 = plt.subplot(3,3,7)\n",
    "df_filtered.plot.scatter(x=\"mean agent measurement noise\",   y=\"scalar param1\",c=\"Accuracy Error\",  ax=ax7);\n",
    "ax8 = plt.subplot(3,3,8)\n",
    "df_filtered.plot.scatter(x=\"std environment noise\",    y=\"scalar param1\",c=\"Accuracy Error\",  ax=ax8);\n",
    "ax9 = plt.subplot(3,3,9)\n",
    "df_filtered.plot.scatter(x=\"mean agent measurement noise\",   y=\"std environment noise\", c=\"Accuracy Error\",  ax=ax9);\n",
    "# df_filtered.plot.hexbin(x=\"scalar param1\",                  y=\"scalar param1\",C=\"Accuracy Error\",  ax=ax9);\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "readFromPickle__T00__MR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5795e6c0ff833bb3ab4f7e7e3eae67d009f557ff55a7b0dec67adb4ab072af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
